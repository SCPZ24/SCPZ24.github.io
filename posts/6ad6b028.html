<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>神经网络数学运算 | SCPZ24-blog</title><meta name="author" content="SCPZ24"><meta name="copyright" content="SCPZ24"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="VectorizationFor computer, doing Loop to handling some iterating tasks is slow. Store some information in vectors, matrices, tensors can be greatly accelerated. Derivative for MatrixGradient for Vecto">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络数学运算">
<meta property="og:url" content="http://example.com/posts/6ad6b028.html">
<meta property="og:site_name" content="SCPZ24-blog">
<meta property="og:description" content="VectorizationFor computer, doing Loop to handling some iterating tasks is slow. Store some information in vectors, matrices, tensors can be greatly accelerated. Derivative for MatrixGradient for Vecto">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/OldWu.png">
<meta property="article:published_time" content="2025-10-18T03:54:00.000Z">
<meta property="article:modified_time" content="2025-12-15T15:13:00.000Z">
<meta property="article:author" content="SCPZ24">
<meta property="article:tag" content="Adam">
<meta property="article:tag" content="Dropout">
<meta property="article:tag" content="反向传播">
<meta property="article:tag" content="梯度">
<meta property="article:tag" content="维度分析">
<meta property="article:tag" content="计算图">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/OldWu.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "神经网络数学运算",
  "url": "http://example.com/posts/6ad6b028.html",
  "image": "http://example.com/img/OldWu.png",
  "datePublished": "2025-10-18T03:54:00.000Z",
  "dateModified": "2025-12-15T15:13:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "SCPZ24",
      "url": "https://SCPZ24.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/lpls.png"><link rel="canonical" href="http://example.com/posts/6ad6b028.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '神经网络数学运算',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="bg-animation" id="web_bg" style="background: linear-gradient(20deg, #fffbeb, #ecfdf5);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">SCPZ24-blog</span></a><a class="nav-page-title" href="/"><span class="site-name">神经网络数学运算</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">神经网络数学运算</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon fas fa-history"></i><span class="post-meta-label">更新于</span><time datetime="2025-12-15T15:13:00.000Z" title="更新于 2025-12-15 23:13:00">2025-12-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-CS224n-%E7%AC%94%E8%AE%B0/">自然语言处理(CS224n)笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h1><p>For computer, doing Loop to handling some iterating tasks is slow. Store some information in vectors, matrices, tensors can be greatly accelerated.</p>
<h1 id="Derivative-for-Matrix"><a href="#Derivative-for-Matrix" class="headerlink" title="Derivative for Matrix"></a>Derivative for Matrix</h1><h2 id="Gradient-for-Vector"><a href="#Gradient-for-Vector" class="headerlink" title="Gradient for Vector"></a>Gradient for Vector</h2><p>Given a function with $1$ output and $n$ inputs(n-dimension vector), we have</p>
<script type="math/tex; mode=display">
f(x)=f(x_1,x_2,......,x_n)</script><p>then</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial x}=[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\frac{\partial f}{\partial x_3},......,\frac{\partial f}{\partial x_n}]</script><h2 id="Jacobian-Matrix"><a href="#Jacobian-Matrix" class="headerlink" title="Jacobian Matrix"></a>Jacobian Matrix</h2><p>The gradient is for a function that inputs $n$ and outputs $1$.<br>The Jacobian Matrix is for a function that inputs $n$ and outputs $m$.<br>We have</p>
<script type="math/tex; mode=display">
\begin{matrix}f(x)=[f_1(x_1,x_2,......,x_n),......,f_m(x_1,x_2,......,x_n)]\\=[f_1(x),f_2(x),......,f_m(x)]\end{matrix}</script><p>then</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial x}=\begin{bmatrix}\frac{\partial f_1}{\partial x_1}......\frac{\partial f_1}{\partial x_n}\\............\\\frac{\partial f_m}{\partial x_1}......\frac{\partial f_m}{\partial x_n}\end{bmatrix}</script><h3 id="Elementwise-Activation-Function"><a href="#Elementwise-Activation-Function" class="headerlink" title="Elementwise Activation Function"></a>Elementwise Activation Function</h3><p>For a activation function $h(x)$ which takes $n$ input and $n$ output, the Jacobian Matrix should be a diagonal $n \times n$ matrix.</p>
<script type="math/tex; mode=display">
(\frac{\partial h}{\partial z})_{ij}=\begin{cases}h'(z_i)\,\,if\,\,\,i=j\\0\,\,\,else\end{cases}</script><h3 id="Examples-for-Jacobian"><a href="#Examples-for-Jacobian" class="headerlink" title="Examples for Jacobian"></a>Examples for Jacobian</h3><script type="math/tex; mode=display">
\frac{\partial(Wx+b)}{\partial x}=W</script><script type="math/tex; mode=display">
\frac{\partial(Wx+b)}{\partial b}=E(Identity Matrix)</script><script type="math/tex; mode=display">
\frac{\partial(u^Th)}{\partial u}=h^T</script><h2 id="Shape-Convention"><a href="#Shape-Convention" class="headerlink" title="Shape Convention"></a>Shape Convention</h2><p>$s$ a single value while $W$ is $n \times m$ matrix.<br>We define that $\frac{\partial s}{\partial W}$ is also a $n\times m$ matrix.<br>By convention, we set the gradient of any object to be the shape of that object.</p>
<h1 id="Appliance-of-Chain-Rule"><a href="#Appliance-of-Chain-Rule" class="headerlink" title="Appliance of Chain Rule"></a>Appliance of Chain Rule</h1><h2 id="The-Chain-Rule"><a href="#The-Chain-Rule" class="headerlink" title="The Chain Rule"></a>The Chain Rule</h2><p>For a neural network with a linear layer, activation layer, and a last linear layer, we have<br>$x(input)$,$z=Wx+b$,$h=f(z)$,$s=u^Th$.<br>In terms of finding $\frac{\partial s}{\partial b}$, we should work out</p>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}\cdot\frac{\partial z}{\partial b}</script><p>We then have</p>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial b}=u^T \text{diag}(f'(z))I=u^T \odot f'(z)</script><p>The symbol $\odot$ means element-wise multiplication between two same-dimensional vectors.</p>
<h2 id="Reusing-Computation"><a href="#Reusing-Computation" class="headerlink" title="Reusing Computation"></a>Reusing Computation</h2><p>For $\frac{\partial s}{\partial W}$, we have</p>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial W}=\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}\cdot\frac{\partial z}{\partial W}</script><p>So, $\frac{\partial s}{\partial W}$ and $\frac{\partial s}{\partial b}$ have same part $\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}$.<br>In regard to they are in a same layer, so the value can be passed from upstream layers and be reused to work out target derivatives.</p>
<h3 id="Dimension-Analyse"><a href="#Dimension-Analyse" class="headerlink" title="Dimension Analyse"></a>Dimension Analyse</h3><p>$W$ is $n\times m$; $x$ is $m\times 1$; $b,z,h,u$ is $n\times 1$.<br>$\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}$ is $1\times n$.<br>$\frac{\partial z}{\partial W}$ is $n\times n\times m$.<br>Them multiplies, the result is $1\times n\times m$, can collapses to $n\times m$, in this cases, meet the definition of the Shape Convention.</p>
<h1 id="Numeric-Gradient"><a href="#Numeric-Gradient" class="headerlink" title="Numeric Gradient"></a>Numeric Gradient</h1><p>Numeric Gradient is used to check whether the algorithm to work out gradients is implemented correctly.<br>Starting from the definition of derivative, we see both sides of a certain value.<br>(for small $h \approx$ <code>1e-4</code> on computer)</p>
<script type="math/tex; mode=display">
f'(x)\approx\frac{f(x+h)-f(x-h)}{2h}</script><h1 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h1><p>While doing forward propagation and backward propagation, we should have a graph to store our chain of calculation.<br><img src="/img/CS224n/截屏2025-10-18 20.39.32.png" alt=""><br>When backward propagation, just go through a reverse order of the graph carrying the current accumulated derivatives.<br><img src="/img/CS224n/截屏2025-10-18 20.41.30.png" alt=""><br>For a single Node case:<br><img src="/img/CS224n/截屏2025-10-18 20.41.59.png" alt=""><br><img src="/img/CS224n/截屏2025-10-18 20.43.25.png" alt=""><br>If the route divides in a node, then the gradient back passed from all routes should be sum up.<br><img src="/img/CS224n/截屏2025-10-18 20.45.15.png" alt=""><br>So in theory, we can build networks not only have classical neurons linked regularly layer by layer, but also like a <strong>DAG</strong> with diverse type of nodes.<br>Just work out gradient for each node. And go through the graph based on a <strong>topological order</strong> of the graph.<br><img src="/img/CS224n/截屏2025-10-18 20.48.06.png" alt=""></p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam is a optimizing method that introduces <strong>Momentum $m$</strong> and <strong>Adaptive Learning Rate $v$</strong>.<br>Given the total parameter set $\theta$ and the hyper parameter $\alpha,\beta_1,\beta_2$, for the learning batch $t$, we modify<br><img src="/img/CS224n/截屏2025-10-21 16.08.40.png" alt=""></p>
<h2 id="Advantage-of-momentum"><a href="#Advantage-of-momentum" class="headerlink" title="Advantage of momentum"></a>Advantage of momentum</h2><p><em>Generated by Doubao</em><br>$m$ is a rolling average of gradients. Instead of relying solely on the noisy gradient of the current minibatch, it combines past gradients. This smooths out the fluctuations in gradients, so parameter updates don’t swing wildly. With lower variance, updates are more stable and consistent, which helps the model converge better and learn more reliably, avoiding being thrown off by the randomness in small batches of data.</p>
<h2 id="Advantage-of-Adaptive-Learning-Rate"><a href="#Advantage-of-Adaptive-Learning-Rate" class="headerlink" title="Advantage of Adaptive Learning Rate"></a>Advantage of Adaptive Learning Rate</h2><p>$v$ is a kind of rolling average of the magnitude(模长) of every gradient.<br><em>Generated by Doubao</em><br>Some parameters might have only had small gradient signals so far (leading to small $v$). By giving these parameters larger updates, Adam ensures they can still adjust significantly and contribute to learning. This way, even parameters that haven’t been changing much get a chance to be updated meaningfully, helping the model learn more comprehensively and efficiently.</p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>Dropout is a regularization technique.<br>For a hidden layer $h$ in the model, to every node, we have possibility $p$ to ignore it.<br>For ignored nodes, they have no output in forward propagation and no update to themselves in backward propagation.<br>If the some nodes are ignored in a layer, the total output of the layer may be smaller(the output signal may be weaker).<br>So, to every surviving node, we multiply $\frac{1}{1-p}$ to its output.</p>
<script type="math/tex; mode=display">
h_{modified\,\,out}=\frac{1}{1-p}h_{origin\,\,out}</script><p>We can use vector multiplication to make process faster instead of using for-loop.</p>
<h2 id="Circumstances-to-Apply-Dropout"><a href="#Circumstances-to-Apply-Dropout" class="headerlink" title="Circumstances to Apply Dropout"></a>Circumstances to Apply Dropout</h2><p>Dropout should only be applied while training.<br>When the model is used to predict, we should not drop out any nodes to throughly use the information stored in the neural network.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://SCPZ24.github.io">SCPZ24</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/6ad6b028.html">http://example.com/posts/6ad6b028.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">SCPZ24-blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%A2%AF%E5%BA%A6/">梯度</a><a class="post-meta__tags" href="/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">反向传播</a><a class="post-meta__tags" href="/tags/%E7%BB%B4%E5%BA%A6%E5%88%86%E6%9E%90/">维度分析</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E5%9B%BE/">计算图</a><a class="post-meta__tags" href="/tags/Adam/">Adam</a><a class="post-meta__tags" href="/tags/Dropout/">Dropout</a></div><div class="post-share"><div class="social-share" data-image="/img/OldWu.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/ade63eae.html" title="RNN进阶"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-24%2023.16.44.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-10-26</div><div class="info-item-2">RNN进阶</div></div><div class="info-2"><div class="info-item-1">Long Short-Term Memory RNNsLSTM introduces some gates into the model.Cell content is the container vector to store long-term memories.The sigmoids in the forget gate, input gate and output gate are used to turn switch on($1$) or off($0$), in order decide which value in the coming vector remains and which discards.For example, if the model judges that a certain dimension of $c^{(t-1)}$ should be forgotten, then the same dimension in $f^{(t)}$ should be nearly $0$. Then after element-wise produ...</div></div></div></a><a class="pagination-related" href="/posts/6fe72dd0.html" title="RNN"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-23%2020.57.19.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-10-23</div><div class="info-item-2">RNN</div></div><div class="info-2"><div class="info-item-1">Language ModelingThe task of a language model is to predict the next word given the formal several context words.For example, when typing some words in a keyboard, the phone will predict some potential next words.Recurrent Neural Network(RNN) is a common tool to build a language model. Brief Structure of RNNA sentence is actually a sequence of words.RNN can take in previous words and predict the upcoming words. Generating WordsViewing the ContextLook up the embedding of each word in the seque...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SCPZ24</div><div class="author-info-description">软件工程大二学生，努力成为极客中。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SCPZ24"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的机器学习导论(MIT6.036)和自然语言处理(CS224n)笔记现已上传。欢迎参考交流。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Vectorization"><span class="toc-number">1.</span> <span class="toc-text">Vectorization</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Derivative-for-Matrix"><span class="toc-number">2.</span> <span class="toc-text">Derivative for Matrix</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-for-Vector"><span class="toc-number">2.1.</span> <span class="toc-text">Gradient for Vector</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Jacobian-Matrix"><span class="toc-number">2.2.</span> <span class="toc-text">Jacobian Matrix</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Elementwise-Activation-Function"><span class="toc-number">2.2.1.</span> <span class="toc-text">Elementwise Activation Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Examples-for-Jacobian"><span class="toc-number">2.2.2.</span> <span class="toc-text">Examples for Jacobian</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Shape-Convention"><span class="toc-number">2.3.</span> <span class="toc-text">Shape Convention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Appliance-of-Chain-Rule"><span class="toc-number">3.</span> <span class="toc-text">Appliance of Chain Rule</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Chain-Rule"><span class="toc-number">3.1.</span> <span class="toc-text">The Chain Rule</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reusing-Computation"><span class="toc-number">3.2.</span> <span class="toc-text">Reusing Computation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dimension-Analyse"><span class="toc-number">3.2.1.</span> <span class="toc-text">Dimension Analyse</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Numeric-Gradient"><span class="toc-number">4.</span> <span class="toc-text">Numeric Gradient</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Computation-Graph"><span class="toc-number">5.</span> <span class="toc-text">Computation Graph</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Adam"><span class="toc-number">6.</span> <span class="toc-text">Adam</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Advantage-of-momentum"><span class="toc-number">6.1.</span> <span class="toc-text">Advantage of momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advantage-of-Adaptive-Learning-Rate"><span class="toc-number">6.2.</span> <span class="toc-text">Advantage of Adaptive Learning Rate</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dropout"><span class="toc-number">7.</span> <span class="toc-text">Dropout</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Circumstances-to-Apply-Dropout"><span class="toc-number">7.1.</span> <span class="toc-text">Circumstances to Apply Dropout</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-12%2011.58.16.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络和树递归神经网络"/></a><div class="content"><a class="title" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络">卷积神经网络和树递归神经网络</a><time datetime="2025-11-12T00:56:00.000Z" title="发表于 2025-11-12 08:56:00">2025-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a7b1f35d.html" title="训练优化"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="训练优化"/></a><div class="content"><a class="title" href="/posts/a7b1f35d.html" title="训练优化">训练优化</a><time datetime="2025-11-07T03:21:00.000Z" title="发表于 2025-11-07 11:21:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8c119005.html" title="HuggingFace入门">HuggingFace入门</a><time datetime="2025-11-02T04:55:00.000Z" title="发表于 2025-11-02 12:55:00">2025-11-02</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2026 By SCPZ24</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="/js/tw_cn.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'ams',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.1.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4"></script></div></div></body></html>