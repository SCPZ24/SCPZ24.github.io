<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>预训练后续 | SCPZ24-blog</title><meta name="author" content="SCPZ24"><meta name="copyright" content="SCPZ24"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Prompting for Pretrained ModelsThis involves no gradient steps!Pretrained models have learned some basic tools and knowledges to handle human language tasks.Take GPT as example. GPT is a decoder that">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练后续">
<meta property="og:url" content="http://example.com/posts/26410c69.html">
<meta property="og:site_name" content="SCPZ24-blog">
<meta property="og:description" content="Prompting for Pretrained ModelsThis involves no gradient steps!Pretrained models have learned some basic tools and knowledges to handle human language tasks.Take GPT as example. GPT is a decoder that">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png">
<meta property="article:published_time" content="2025-11-01T06:32:00.000Z">
<meta property="article:modified_time" content="2025-11-12T06:29:00.000Z">
<meta property="article:author" content="SCPZ24">
<meta property="article:tag" content="DPO">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Prompting">
<meta property="article:tag" content="模型微调">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "预训练后续",
  "url": "http://example.com/posts/26410c69.html",
  "image": "http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png",
  "datePublished": "2025-11-01T06:32:00.000Z",
  "dateModified": "2025-11-12T06:29:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "SCPZ24",
      "url": "https://SCPZ24.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/lpls.png"><link rel="canonical" href="http://example.com/posts/26410c69.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '预训练后续',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="bg-animation" id="web_bg" style="background: linear-gradient(20deg, #fffbeb, #ecfdf5);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">SCPZ24-blog</span></a><a class="nav-page-title" href="/"><span class="site-name">预训练后续</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">预训练后续</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon fas fa-history"></i><span class="post-meta-label">更新于</span><time datetime="2025-11-12T06:29:00.000Z" title="更新于 2025-11-12 14:29:00">2025-11-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-CS224n-%E7%AC%94%E8%AE%B0/">自然语言处理(CS224n)笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Prompting-for-Pretrained-Models"><a href="#Prompting-for-Pretrained-Models" class="headerlink" title="Prompting for Pretrained Models"></a>Prompting for Pretrained Models</h1><p><em>This involves no gradient steps!</em><br>Pretrained models have learned some basic tools and knowledges to handle human language tasks.<br>Take GPT as example. GPT is a decoder that can generated a sequence of tokens based on input tokens(may be human’s questions).<br>When it’s not finetuned for specific tasks, we can still use some prompts(ask it explicitly in the input sentence) to let it handle specific tasks with already learned general skills.<br>The state that the model is trained on enough amount of data and is big enough, then it can handle some specific task without finetuning is called <strong>Emergent</strong>.</p>
<h2 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h2><p>Give the model a prompt, and just ask following questions.<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Your following task is to translate the sentences to Chinese.</span><br><span class="line"></span><br><span class="line">I love china.</span><br><span class="line">I want to eat fish.</span><br></pre></td></tr></table></figure><br>Then, the model can generate response that translates user’s request.</p>
<h2 id="Few-shot-Learning"><a href="#Few-shot-Learning" class="headerlink" title="Few-shot Learning"></a>Few-shot Learning</h2><p>Give the model a prompt, and some example inputs and outputs. Then the model can do the task more precisely.<br>A shot is an example for the model.<br><img src="/img/CS224n/截屏2025-11-01 22.17.45.png" alt=""><br>The picture shows the user gives the model a task and give some examples. And let the model to do new task of the same kind.</p>
<h3 id="Chain-of-Thought-Prompting"><a href="#Chain-of-Thought-Prompting" class="headerlink" title="Chain-of-Thought Prompting"></a>Chain-of-Thought Prompting</h3><p>For some tasks that needs a chain of thought(can’t work out directly), we can give the examples in the form of Question-Thinking Chain-Answer.<br><img src="/img/CS224n/截屏2025-11-01 22.21.10.png" alt=""><br>In this pattern, the model tends to give more precise answer with its thinking chain.<br>Another way to implement Chain-of-Thought is force the model to output a sentence like “Let’s think step by step.”<br>Then the model will output step-by-step thinking.</p>
<h1 id="Scaling-Up-Finetuning"><a href="#Scaling-Up-Finetuning" class="headerlink" title="Scaling Up Finetuning"></a>Scaling Up Finetuning</h1><p>We can feed the data each in the form of <strong>Initial Input-Task Label-Output</strong>.<br>Then we can let the model do multi-tasks.<br>This finetuning doesn’t need the model to do Chatting with user, but do various kinds of tasks with clear label in the input.</p>
<h1 id="Instruction-Finetuning"><a href="#Instruction-Finetuning" class="headerlink" title="Instruction Finetuning"></a>Instruction Finetuning</h1><h2 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h2><p><img src="/img/CS224n/截屏2025-11-01 23.18.39.png" alt=""><br>Finetuning the model with the dataset in the from of <strong>Question-Answer</strong>.<br><img src="/img/CS224n/截屏2025-11-01 23.18.56.png" alt=""></p>
<h2 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h2><p>language modeling penalizes all token-level mistakes equally. But tasks like open-ended creative generation have no right answer, and even with instruction finetuning, there are mismatches between the LM objective and the objective of “satisfy human preferences”.</p>
<h1 id="Direct-Preference-Optimization-DPO"><a href="#Direct-Preference-Optimization-DPO" class="headerlink" title="Direct Preference Optimization(DPO)"></a>Direct Preference Optimization(DPO)</h1><p>After doing the instruction finetuning, we can do some extra optimization for the model, to let the model more possible to generate the answer that satisfy humans.</p>
<h2 id="Dataset-Demand"><a href="#Dataset-Demand" class="headerlink" title="Dataset Demand"></a>Dataset Demand</h2><p>We need the data in the form of</p>
<ul>
<li>Input sequence $x$</li>
<li>Some possible output sequence $y$s to the given $x$</li>
<li>The human labeled satisfied rank of the $y$s.<br>The model itself can calculate the log-likelihood $p_\theta(y|x)$ for an input $x$ and a specific $y$ as follows<script type="math/tex; mode=display">
\log p_\theta(y|x)=\log\prod_{t=1}^np_\theta(y_t|x,y_1,y_2,...,y_{t-1})=\sum_{t=1}^n\log p_\theta(y_t|x,y_1,y_2,...,y_{t-1})</script><em>Note that $y$ is a token sequence here. $y_i$ is the $i^{th}$ token.</em><h2 id="The-Loss-to-Optimize"><a href="#The-Loss-to-Optimize" class="headerlink" title="The Loss to Optimize"></a>The Loss to Optimize</h2>We can have a reward model to evaluate the satisfying score of a pair of $x,y$ which can be represented as $RM(x,y)$. Use this model to help the original model to optimize.<br>But in the pattern of DPO, during the calculation, we can cancel out all the $RM(x,y)$.<br>For a $x$ and its $y$s, we can take out all pairs $y^w$ and $y^l$ that $y^w$ is more satisfying than $y^l$ for humans<br>Then we can have the DPO Loss<script type="math/tex; mode=display">
J_{DPO}(\theta)=-E_{(x,y^w,y^l)\sim D}[\log\sigma(RM_\theta(x,y^w)-RM_\theta(x,y^l))]</script>in which, $E<em>{(x,y^w,y^l)\sim D}$ means sampling and out all pairs of demanding $y^w,y^l$ in dataset add up the calculating result.<br>$RM</em>\theta(x,y)$ is the division of original model’s(right after finetuning) predicted possibility to current-step model’s(the model updates its parameters every gradient step) predicted possibility. <em>So we need to remain a copy of the original model.</em><script type="math/tex; mode=display">
RM_\theta=\beta\log\frac{p_\theta^{RL}(y|x)}{p^{PT}(y|x)}</script>$\beta$ is the <strong>Temperature Coefficient</strong>, a hyper-parameter scalar.<br>$p_\theta^{RL}$ is the current model prediction(after some turns of parameter update).<br>$p^{PT}$ is the original model prediction(right after the finetuning process).<br>Using this form of division can prevent the model goes too far away from the finetuned version, generating unexpected answers.</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://SCPZ24.github.io">SCPZ24</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/26410c69.html">http://example.com/posts/26410c69.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">SCPZ24-blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DPO/">DPO</a><a class="post-meta__tags" href="/tags/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">模型微调</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Prompting/">Prompting</a></div><div class="post-share"><div class="social-share" data-image="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/b1811702.html" title="预训练"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-02-04</div><div class="info-item-2">预训练</div></div><div class="info-2"><div class="info-item-1">SubwordA trained embedding can’t cover every word in human language(humans are keeping generating new words, and there misspelling of human).If we keep signing this words with &lt;Unk&gt;(unknown) token, then great amount of information will be missed. Byte-pair Encode AlgorithmThen, we can generate some subword tokens according to the appearing frequency in the corpus of specific character combinations.In detail, we do  Separate the whole text into smallest units(usually one character). Each...</div></div></div></a><a class="pagination-related" href="/posts/caaad70d.html" title="Embedding"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-13%2019.16.55.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-10-23</div><div class="info-item-2">Embedding</div></div><div class="info-2"><div class="info-item-1">Distributed RepresentationNaturally, we use context to learn about word.So, we can build a dense vector for each word in corpus(word library/vocabulary).By dot product, we can detect the similarity of given two words. Large result stands for large similarity while small one stands for small similarity.Usually, a word vector is 300~500 dimensional. For some modern models, it can be around 1000. Word2VectorWord2Vector is the algorithm to turn words in corpus to vectors.There are two methods to ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SCPZ24</div><div class="author-info-description">软件工程大二学生，努力成为极客中。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SCPZ24"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的机器学习导论(MIT6.036)和自然语言处理(CS224n)笔记现已上传。欢迎参考交流。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Prompting-for-Pretrained-Models"><span class="toc-number">1.</span> <span class="toc-text">Prompting for Pretrained Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Zero-shot-Learning"><span class="toc-number">1.1.</span> <span class="toc-text">Zero-shot Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Few-shot-Learning"><span class="toc-number">1.2.</span> <span class="toc-text">Few-shot Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Chain-of-Thought-Prompting"><span class="toc-number">1.2.1.</span> <span class="toc-text">Chain-of-Thought Prompting</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scaling-Up-Finetuning"><span class="toc-number">2.</span> <span class="toc-text">Scaling Up Finetuning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Instruction-Finetuning"><span class="toc-number">3.</span> <span class="toc-text">Instruction Finetuning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Implement"><span class="toc-number">3.1.</span> <span class="toc-text">Implement</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Limitation"><span class="toc-number">3.2.</span> <span class="toc-text">Limitation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Direct-Preference-Optimization-DPO"><span class="toc-number">4.</span> <span class="toc-text">Direct Preference Optimization(DPO)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset-Demand"><span class="toc-number">4.1.</span> <span class="toc-text">Dataset Demand</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Loss-to-Optimize"><span class="toc-number">4.2.</span> <span class="toc-text">The Loss to Optimize</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-12%2011.58.16.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络和树递归神经网络"/></a><div class="content"><a class="title" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络">卷积神经网络和树递归神经网络</a><time datetime="2025-11-12T00:56:00.000Z" title="发表于 2025-11-12 08:56:00">2025-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a7b1f35d.html" title="训练优化"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="训练优化"/></a><div class="content"><a class="title" href="/posts/a7b1f35d.html" title="训练优化">训练优化</a><time datetime="2025-11-07T03:21:00.000Z" title="发表于 2025-11-07 11:21:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8c119005.html" title="HuggingFace入门">HuggingFace入门</a><time datetime="2025-11-02T04:55:00.000Z" title="发表于 2025-11-02 12:55:00">2025-11-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2026 By SCPZ24</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="/js/tw_cn.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'ams',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.1.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4"></script></div></div></body></html>