<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformers | SCPZ24-blog</title><meta name="author" content="SCPZ24"><meta name="copyright" content="SCPZ24"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="For RNNs, it’s difficult to handle word that are far from each other but have dependencies.Moreover, GPUs work well in doing parallelizable tasks such as matrix multiplication. For recurrent jobs, GPU">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformers">
<meta property="og:url" content="http://example.com/posts/ab3e853d.html">
<meta property="og:site_name" content="SCPZ24-blog">
<meta property="og:description" content="For RNNs, it’s difficult to handle word that are far from each other but have dependencies.Moreover, GPUs work well in doing parallelizable tasks such as matrix multiplication. For recurrent jobs, GPU">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-28%2019.47.08.png">
<meta property="article:published_time" content="2025-10-27T03:06:00.000Z">
<meta property="article:modified_time" content="2025-12-17T04:48:00.000Z">
<meta property="article:author" content="SCPZ24">
<meta property="article:tag" content="RoPE">
<meta property="article:tag" content="Seq2seq">
<meta property="article:tag" content="Transformers">
<meta property="article:tag" content="残差">
<meta property="article:tag" content="注意力机制">
<meta property="article:tag" content="维度分析">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-28%2019.47.08.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers",
  "url": "http://example.com/posts/ab3e853d.html",
  "image": "http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-28%2019.47.08.png",
  "datePublished": "2025-10-27T03:06:00.000Z",
  "dateModified": "2025-12-17T04:48:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "SCPZ24",
      "url": "https://SCPZ24.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/lpls.png"><link rel="canonical" href="http://example.com/posts/ab3e853d.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformers',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="bg-animation" id="web_bg" style="background: linear-gradient(20deg, #fffbeb, #ecfdf5);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-28%2019.47.08.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">SCPZ24-blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformers</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformers</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon fas fa-history"></i><span class="post-meta-label">更新于</span><time datetime="2025-12-17T04:48:00.000Z" title="更新于 2025-12-17 12:48:00">2025-12-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-CS224n-%E7%AC%94%E8%AE%B0/">自然语言处理(CS224n)笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>For RNNs, it’s difficult to handle word that are far from each other but have dependencies.<br>Moreover, GPUs work well in doing parallelizable tasks such as matrix multiplication. For recurrent jobs, GPUs can’t work well.<br><strong>Transformers</strong> can take advantages of parallel computing of GPUs.<br>Time cost for transformers is $O(dn^2)$ while for RNN is $O(d^2n)$.<br>In which, $d$ is the dimension of word vectors and $n$ is the sequence length.</p>
<h1 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h1><h2 id="Similarity-with-RNN’s-Attention"><a href="#Similarity-with-RNN’s-Attention" class="headerlink" title="Similarity with RNN’s Attention"></a>Similarity with RNN’s Attention</h2><p>Recall the Attention process in <a href="/posts/ade63eae.html" title="RNN进阶">RNN进阶</a><br>The whole process of <strong>Query</strong>-<strong>Key</strong>-<strong>Value</strong>.</p>
<script type="math/tex; mode=display">
e_i=s_t^T(U^TV)h_i=(Us_t)^T(Vh_i)</script><p>To the processing token $s_t$, we map it with $U$, and get a query vector of $s_t$.<br>Then mapping $h_i$ with $V$ is the process of get the key.<br>Multiply query vector and key vector, we get the weight for each value.</p>
<script type="math/tex; mode=display">
\alpha^t=softmax(e^t)</script><script type="math/tex; mode=display">
a_t=\sum_{i=1}^N\alpha_i^tv_i</script><h2 id="Q-K-V-for-Transformers"><a href="#Q-K-V-for-Transformers" class="headerlink" title="Q-K-V for Transformers"></a>Q-K-V for Transformers</h2><p>Let $w_{1:n}$ be a sequence of words in corpus $V$.<br>$E$ is the whole word Embeddings $d\times |V|$.<br>For each word $w_i$, we have its word vector $x_i(d\times 1)=Ew_i$.<br>Then, we have three mapping matrices $Q,K,V$, which are all $d\times d$.<br>For every word in the sentence $w_i$, we take the mapping</p>
<script type="math/tex; mode=display">
q_i=Qx_i</script><script type="math/tex; mode=display">
k_i=Kx_i</script><script type="math/tex; mode=display">
v_i=Vx_i</script><p>Then, we can build a attention distribution from any one word $i$ to all words in the sentence.</p>
<script type="math/tex; mode=display">
e_{ij}=q_i^Tk_j</script><p>Use softmax to map the results to a distribution</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{\exp(e_{ij})}{\sum_{j^{'}}\exp(e_{ij^{'}})}</script><p>Do a weighted sum of output softmax</p>
<script type="math/tex; mode=display">
o_i=\sum_ja_{ij}v_j</script><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>The math process can be vectorized.<br>Let $X=[x_1;x_2;……;x_n]$ which is a $n\times d$ matrix that holds the all word embeddings.<br>The same with before, we have $Q,K,V\in R^{d\times d}$.<br>So we have $XQ,XK,XV\in R^{n\times d}$.<br>Then we can work out the output $o$ of this Q-K-V layer directly.</p>
<script type="math/tex; mode=display">
o=softmax(XQ(XK)^T)XV</script><p>It’s really fast for GPUs.</p>
<h2 id="Piling-Up-for-more-Layers"><a href="#Piling-Up-for-more-Layers" class="headerlink" title="Piling Up for more Layers"></a>Piling Up for more Layers</h2><p>A layer of QKV takes in a sequence of word vectors and outputs a same-length sequence of word vectors.<br>Only one layer can’t work the best. We can use more layers of QKV self-attention.<br>But in fact, there are no element-wise nonlinearities so far. In math, stacking more such layers are just re-averaging the <strong>value vectors</strong>.<br>The solution is to add a <strong>Feed-forward Network</strong> to post-process each output vector.</p>
<script type="math/tex; mode=display">
m_i=MLP(o_i)=W_2ReLU(W_1o_i+b_1)+b_2</script><p>So the big picture looks like<br><img src="/img/CS224n/截屏2025-10-27 23.33.03.png" alt=""><br>We can call a layer of attention(including Q-K-V and Feed-forward Network) a Transformer <strong>Building Block</strong>.</p>
<h1 id="Sequence-Order"><a href="#Sequence-Order" class="headerlink" title="Sequence Order"></a>Sequence Order</h1><p>In math, in the process of Q-K-V, every word is dealt with equally.<br>In another word, no matter in what order the original sequence is, the outputs are all the same.</p>
<h2 id="Sequence-Index-Vector"><a href="#Sequence-Index-Vector" class="headerlink" title="Sequence Index Vector"></a>Sequence Index Vector</h2><p>We need to give the model a notation of sequence.<br>To trade off, we have to fix the max length of the input sequence to $n$, and pad the rest places with default vectors.<br>We initialize $n$ $d\times 1$ vectors $p_i$ for $i\in \begin{Bmatrix}1,2,……,n\end{Bmatrix}$.<br>For the input word vector $x_i$ in index $i$, we add the $p_i$.</p>
<script type="math/tex; mode=display">
\tilde x_i=x_i+p_i</script><p>Pass the modified vector $\tilde x$ to the self-attention layer.<br>We can learn all the $p_i$ while training.<br>Then, each $p_i$ is a distinct sign for every index, which can be used to represent the sequence order.</p>
<h2 id="Rotary-Position-Embedding-RoPE"><a href="#Rotary-Position-Embedding-RoPE" class="headerlink" title="Rotary Position Embedding(RoPE)"></a>Rotary Position Embedding(RoPE)</h2><h3 id="2-d-Vector-Rotation"><a href="#2-d-Vector-Rotation" class="headerlink" title="2-d Vector Rotation"></a>2-d Vector Rotation</h3><p>To a 2-d vector $v=(a,b)$, we have its complex $c_v=a+bi$.<br>Then $e^{i\theta}c_v$ is the complex of the vector that is the $v$ rotated by angle $\theta$.<br>The process can also be replaced by a matrix multiplication(a $2\times 2$ matrix multiplies a $2\times 1$ vector).<br>If two 2-d vectors are rotated by a same angle $\theta$, their inner product doesn’t change.<br>If $v_1$ is rotated $\theta_1$ and $v_2$ rotated $\theta_2$, the only value that influences the their inner product  is $\theta_1-\theta_2$.</p>
<h3 id="Rotation-of-High-dimension-Vectors-for-Transformers"><a href="#Rotation-of-High-dimension-Vectors-for-Transformers" class="headerlink" title="Rotation of High-dimension Vectors for Transformers"></a>Rotation of High-dimension Vectors for Transformers</h3><p><img src="/img/CS224n/截屏2025-10-28 09.52.06.png" alt=""><br>Break the $d$-dimension vector into $d/2$ $2$-dimension vectors.<br>For each $2$-dimension vector, do rotation respectively.<br>Then concatenate all the results back to a $d$-dimension vector.<br>This process can also be vectorized.</p>
<h3 id="Rotate-for-Sequence"><a href="#Rotate-for-Sequence" class="headerlink" title="Rotate for Sequence"></a>Rotate for Sequence</h3><p>For every $q_i$ and $k_i$, we rotate them with a angle $\theta_i = mi$, in which $m$ is a constant, and $i$ is the index of their token in the original sentence.<br>So, the sequence order of the input sentence is perfectly described.<br>Note that what matters in this process is the distance of two words in the sentence(<strong>Relative Position</strong>) instead of the <strong>Absolute Position</strong> of each word.</p>
<h1 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h1><p>To use self-attention in decoders, the model should not know the future while predicting next words.<br>We only take out the already generated words to Query.<br>While dealing with future words, we mask them by setting the attention score to $-\infty$.</p>
<script type="math/tex; mode=display">
e_{ij}=\begin{cases}q_i^Tk_j,j\leq i\\-\infty,j>i\end{cases}</script><p><img src="/img/CS224n/截屏2025-10-28 10.25.58.png" alt=""><br>Also, instead of setting $-\infty$ scores, we can pad some <code>&lt;EMPTY&gt;</code> tokens to the blank that haven’t been generated yet.</p>
<h1 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h1><h2 id="Independent-Q-K-V-s"><a href="#Independent-Q-K-V-s" class="headerlink" title="Independent $Q,K,V$s"></a>Independent $Q,K,V$s</h2><p>In self-attention, we only have a group of $Q,K,V$.<br>The Multi-head Self-attention introduces several groups of $Q_l,K_l,V_l$s.<br>Each group extract information from the input features independently(some catches syntax and some catches dependency for example).</p>
<h2 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h2><p>For word vectors with dimension $d$, and $h$ heads, we let</p>
<script type="math/tex; mode=display">
Q_l,K_l,V_l\in R^{d\times \frac{d}{h}}</script><p>in which $l$ ranges from $1$ to $h$.<br>Each attention has head performs attention and give output independently</p>
<script type="math/tex; mode=display">
o_i=softmax(XQ_l(XK_l)^T)XV_l</script><p>$XQ_l,XK_l,XV_l$ are all $n\times \frac{d}{h}$.<br>So it is $n\times n$ in the softmax.<br>Note that softmax always do the mapping along the last axis. For this matrix, it takes each row in the matrix as input and outputs a mapped row.<br>$o_i$ is $n\times \frac{d}{h}$.<br>Concatenate all $o_i$ for $i\in\begin{Bmatrix}1,2,……,h\end{Bmatrix}$, we get an output $n\times d$ vector. Them objective with a $d\times d$ matrix $Y$(can help integrate outcomes from different heads).</p>
<script type="math/tex; mode=display">
o=[o_1,o_2,......,o_h]Y\in R^{n\times d}</script><h2 id="Vectorization-1"><a href="#Vectorization-1" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>Take $Q$ as an example.<br>First, concatenate all $Q_l$ to form a $d\times d$ matrix $Q$.<br>Compute $XQ$, which is $n\times d$.<br>Break the second dimension down to 2 new dimensions, then the shape goes to $n\times h\times\frac{d}{h}$.<br>Transpose the tensor to $h\times n\times\frac{d}{h}$.<br>The $h$ is the “Heads” axis(piles up the matrices of each heads).<br>Do the same operations to $K$ and $V$.<br><img src="/img/CS224n/截屏2025-10-28 11.43.06.png" alt=""><br>The whole expression is still</p>
<script type="math/tex; mode=display">
o=softmax(XQ(XK)^T)XVY</script><p>Now, $XQ$ is $h\times n\times\frac{d}{h}$. $(XK)^T$ is $h\times\frac{d}{h}\times n$.<br>$XQ(XK)^T$ is $h\times n\times n$.<br>Still, softmax is done to the last dimension.<br><em>View the tensor $XQ(XK)^T$ as a $h$-floor building. On each floor there is a distinct matrix. The softmax is done to each row in each floor.</em><br>Then we multiply it with $XV$ which is also $h\times n\times\frac{d}{h}$.<br>The output is a $h\times n\times\frac{d}{h}$ tensor.<br>Then, pile the first and third dimension up(just like the building fall horizontally) and form a $n\times d$ matrix.<br>Multiply it with $Y$ to get the outcome of this layer.<br><em>The $Y$ are all optional.</em></p>
<h2 id="Scaled-Dot-Product"><a href="#Scaled-Dot-Product" class="headerlink" title="Scaled Dot Product"></a>Scaled Dot Product</h2><p>As $d$ becomes larger, dot products between vectors tend to become large.<br>We should rescale it in each attention layer.<br>So, the update version of attention layer is</p>
<script type="math/tex; mode=display">
o=softmax(\frac{XQ(XK)^T}{\sqrt{d/h}})XVY</script><h1 id="Transformer-Architecture"><a href="#Transformer-Architecture" class="headerlink" title="Transformer Architecture"></a>Transformer Architecture</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><p>The basic architecture is after several building blocks, we connect to Linear Layer and Softmax Layer to get some outputs.<br><img src="/img/CS224n/截屏2025-10-28 18.15.48.png" alt=""></p>
<h2 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h2><p>Before and After a Layer, we have $X^{(i-1)}$ and $X^{(i)}$, we have<br><img src="/img/CS224n/截屏2025-10-28 18.49.39.png" alt=""></p>
<script type="math/tex; mode=display">
X^{(i)}=Layer(X^{(i-1)})</script><p>The additional Residual Connection links directly $X^{(i-1)}$ and $X^{(i)}$. Which makes training more smooth.<br><img src="/img/CS224n/截屏2025-10-28 18.52.18.png" alt=""></p>
<script type="math/tex; mode=display">
X^{(i)}=Layer(X^{(i-1)})+X^{(i-1)}</script><p><img src="/img/CS224n/截屏2025-10-28 18.55.02.png" alt=""><br>With some residual connections, we are more likely to find the global optimize by gradient descent.<br><img src="/img/CS224n/截屏2025-10-28 18.54.08.png" alt=""></p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>After each layer, some word vectors may be larger in scale than other vector.<br>Then, we should do a independent normalization for each word.<br>For each word’s vector $x\in R^d$.<br>Let the mean</p>
<script type="math/tex; mode=display">
\mu=\frac{1}{d}\sum_{i=1}^dx_i</script><p>and the standard deviation</p>
<script type="math/tex; mode=display">
\sigma=\sqrt{\frac{1}{d}\sum_{i=1}^d(x_i-\mu)^2}</script><p>Then, we standardize the initial $x$ and use $\gamma,\beta\in R^d$ to map it.</p>
<script type="math/tex; mode=display">
o_x=\frac{x-\mu}{\sqrt{\sigma}+\epsilon}\odot\gamma+\beta</script><p>in which, $\epsilon$ is a small scalar to prevent against divide zero problem.</p>
<h2 id="Encoder-and-Decoder"><a href="#Encoder-and-Decoder" class="headerlink" title="Encoder and Decoder"></a>Encoder and Decoder</h2><p>The encoder needs no mask. It needs to generate features based on whole inputs.<br><img src="/img/CS224n/截屏2025-10-28 19.40.55.png" alt=""><br>The decoder needs mask.<br><img src="/img/CS224n/截屏2025-10-28 19.36.37.png" alt=""></p>
<h2 id="Cross-attention"><a href="#Cross-attention" class="headerlink" title="Cross-attention"></a>Cross-attention</h2><p><img src="/img/CS224n/截屏2025-10-28 19.47.08.png" alt=""><br>Suppose in the current state, the decoder has generated $t$ tokens $z_1,z_2,……,z_t$. We concatenate these $d$-dimensional vectors to generate a whole matrix $Z=[z_1;z_2;……,z_t]\in R^{t\times d}$.<br>The encoder has $T$ tokens $h_1,h_2,……,h_T$. We concatenate these $d$-dimensional vectors to generate a whole matrix $H=[h_1;h_2;……;h_T]\in R^{T\times d}$.<br>Now, we use $Z$ to Query, and use $H$ to Key and Value.<br>First, we calculate $ZQ$, which is $t\times d$.<br>Then, calculate $HK$ which is $T\times d$.<br>Multiply $ZQ(HK)^T$, which is $t\times T$. Each row refers to the all attention scores in terms of each $z_i$. This indicates which input token should each output(docoder-generated) token should focus more in.<br>Do softmax to each row to get the attention distribution, multiply the result matrix with $HV$ and get the output.<br>The whole process is described as</p>
<script type="math/tex; mode=display">
o=softmax(ZQ(HK)^T)HV</script><p>Sometimes, we can pad some tokens to make the number of decoder features from $t$ tokens to $T$ tokens which is the same as encoder(like masking).</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://SCPZ24.github.io">SCPZ24</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/ab3e853d.html">http://example.com/posts/ab3e853d.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">SCPZ24-blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a><a class="post-meta__tags" href="/tags/Seq2seq/">Seq2seq</a><a class="post-meta__tags" href="/tags/%E7%BB%B4%E5%BA%A6%E5%88%86%E6%9E%90/">维度分析</a><a class="post-meta__tags" href="/tags/Transformers/">Transformers</a><a class="post-meta__tags" href="/tags/RoPE/">RoPE</a><a class="post-meta__tags" href="/tags/%E6%AE%8B%E5%B7%AE/">残差</a></div><div class="post-share"><div class="social-share" data-image="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-28%2019.47.08.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/ade63eae.html" title="RNN进阶"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-24%2023.16.44.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-10-26</div><div class="info-item-2">RNN进阶</div></div><div class="info-2"><div class="info-item-1">Long Short-Term Memory RNNsLSTM introduces some gates into the model.Cell content is the container vector to store long-term memories.The sigmoids in the forget gate, input gate and output gate are used to turn switch on($1$) or off($0$), in order decide which value in the coming vector remains and which discards.For example, if the model judges that a certain dimension of $c^{(t-1)}$ should be forgotten, then the same dimension in $f^{(t)}$ should be nearly $0$. Then after element-wise produ...</div></div></div></a><a class="pagination-related" href="/posts/6ad6b028.html" title="神经网络数学运算"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-12-15</div><div class="info-item-2">神经网络数学运算</div></div><div class="info-2"><div class="info-item-1">VectorizationFor computer, doing Loop to handling some iterating tasks is slow. Store some information in vectors, matrices, tensors can be greatly accelerated. Derivative for MatrixGradient for VectorGiven a function with $1$ output and $n$ inputs(n-dimension vector), we have  f(x)=f(x_1,x_2,......,x_n)then  \frac{\partial f}{\partial x}=[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\frac{\partial f}{\partial x_3},......,\frac{\partial f}{\partial x_n}]Jacobian MatrixThe g...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SCPZ24</div><div class="author-info-description">软件工程大二学生，努力成为极客中。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SCPZ24"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的机器学习导论(MIT6.036)和自然语言处理(CS224n)笔记现已上传。欢迎参考交流。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Self-attention"><span class="toc-number">1.</span> <span class="toc-text">Self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Similarity-with-RNN%E2%80%99s-Attention"><span class="toc-number">1.1.</span> <span class="toc-text">Similarity with RNN’s Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-K-V-for-Transformers"><span class="toc-number">1.2.</span> <span class="toc-text">Q-K-V for Transformers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorization"><span class="toc-number">1.3.</span> <span class="toc-text">Vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Piling-Up-for-more-Layers"><span class="toc-number">1.4.</span> <span class="toc-text">Piling Up for more Layers</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sequence-Order"><span class="toc-number">2.</span> <span class="toc-text">Sequence Order</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Sequence-Index-Vector"><span class="toc-number">2.1.</span> <span class="toc-text">Sequence Index Vector</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Rotary-Position-Embedding-RoPE"><span class="toc-number">2.2.</span> <span class="toc-text">Rotary Position Embedding(RoPE)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-d-Vector-Rotation"><span class="toc-number">2.2.1.</span> <span class="toc-text">2-d Vector Rotation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rotation-of-High-dimension-Vectors-for-Transformers"><span class="toc-number">2.2.2.</span> <span class="toc-text">Rotation of High-dimension Vectors for Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rotate-for-Sequence"><span class="toc-number">2.2.3.</span> <span class="toc-text">Rotate for Sequence</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Masking"><span class="toc-number">3.</span> <span class="toc-text">Masking</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Multi-head-Self-attention"><span class="toc-number">4.</span> <span class="toc-text">Multi-head Self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Independent-Q-K-V-s"><span class="toc-number">4.1.</span> <span class="toc-text">Independent $Q,K,V$s</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implement"><span class="toc-number">4.2.</span> <span class="toc-text">Implement</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorization-1"><span class="toc-number">4.3.</span> <span class="toc-text">Vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scaled-Dot-Product"><span class="toc-number">4.4.</span> <span class="toc-text">Scaled Dot Product</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-Architecture"><span class="toc-number">5.</span> <span class="toc-text">Transformer Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic"><span class="toc-number">5.1.</span> <span class="toc-text">Basic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Residual-Connection"><span class="toc-number">5.2.</span> <span class="toc-text">Residual Connection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Layer-Normalization"><span class="toc-number">5.3.</span> <span class="toc-text">Layer Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-and-Decoder"><span class="toc-number">5.4.</span> <span class="toc-text">Encoder and Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-attention"><span class="toc-number">5.5.</span> <span class="toc-text">Cross-attention</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-12%2011.58.16.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络和树递归神经网络"/></a><div class="content"><a class="title" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络">卷积神经网络和树递归神经网络</a><time datetime="2025-11-12T00:56:00.000Z" title="发表于 2025-11-12 08:56:00">2025-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a7b1f35d.html" title="训练优化"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="训练优化"/></a><div class="content"><a class="title" href="/posts/a7b1f35d.html" title="训练优化">训练优化</a><time datetime="2025-11-07T03:21:00.000Z" title="发表于 2025-11-07 11:21:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8c119005.html" title="HuggingFace入门">HuggingFace入门</a><time datetime="2025-11-02T04:55:00.000Z" title="发表于 2025-11-02 12:55:00">2025-11-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-28%2019.47.08.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2026 By SCPZ24</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="/js/tw_cn.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'ams',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.1.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4"></script></div></div></body></html>