<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>预训练 | SCPZ24-blog</title><meta name="author" content="SCPZ24"><meta name="copyright" content="SCPZ24"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SubwordA trained embedding can’t cover every word in human language(humans are keeping generating new words, and there misspelling of human).If we keep signing this words with &lt;Unk&gt;(unknown) tok">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练">
<meta property="og:url" content="http://example.com/posts/b1811702.html">
<meta property="og:site_name" content="SCPZ24-blog">
<meta property="og:description" content="SubwordA trained embedding can’t cover every word in human language(humans are keeping generating new words, and there misspelling of human).If we keep signing this words with &lt;Unk&gt;(unknown) tok">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png">
<meta property="article:published_time" content="2025-10-31T13:07:00.000Z">
<meta property="article:modified_time" content="2025-02-04T12:22:00.000Z">
<meta property="article:author" content="SCPZ24">
<meta property="article:tag" content="BERT">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Transformers">
<meta property="article:tag" content="词嵌入">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "预训练",
  "url": "http://example.com/posts/b1811702.html",
  "image": "http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png",
  "datePublished": "2025-10-31T13:07:00.000Z",
  "dateModified": "2025-02-04T12:22:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "SCPZ24",
      "url": "https://SCPZ24.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/lpls.png"><link rel="canonical" href="http://example.com/posts/b1811702.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '预训练',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="bg-animation" id="web_bg" style="background: linear-gradient(20deg, #fffbeb, #ecfdf5);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">SCPZ24-blog</span></a><a class="nav-page-title" href="/"><span class="site-name">预训练</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">预训练</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon fas fa-history"></i><span class="post-meta-label">更新于</span><time datetime="2025-02-04T12:22:00.000Z" title="更新于 2025-02-04 20:22:00">2025-02-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-CS224n-%E7%AC%94%E8%AE%B0/">自然语言处理(CS224n)笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Subword"><a href="#Subword" class="headerlink" title="Subword"></a>Subword</h1><p>A trained embedding can’t cover every word in human language(humans are keeping generating new words, and there misspelling of human).<br>If we keep signing this words with <code>&lt;Unk&gt;</code>(unknown) token, then great amount of information will be missed.</p>
<h2 id="Byte-pair-Encode-Algorithm"><a href="#Byte-pair-Encode-Algorithm" class="headerlink" title="Byte-pair Encode Algorithm"></a>Byte-pair Encode Algorithm</h2><p>Then, we can generate some subword tokens according to the appearing frequency in the corpus of specific character combinations.<br>In detail, we do</p>
<ol>
<li>Separate the whole text into smallest units(usually one character). Each adding a special sign(e.g. <code>&lt;/w&gt;</code>) to stand for the end of the word.</li>
<li>Count the appearing frequency of all the adjacent characters. Use most common adjacent characters to form a subword.(e.g. <code>s,u,b</code> are usually appearing together so we add <code>sub</code> as a new subword)</li>
<li>Replace instances of the character pair with the new subword; repeat until desired vocab size.<br>Below is the process of forming subword <code>est</code> as an example<br><img src="/img/CS224n/截屏2025-10-31 21.30.46.png" alt=""><h1 id="Inspiration-of-Pretraining"><a href="#Inspiration-of-Pretraining" class="headerlink" title="Inspiration of Pretraining"></a>Inspiration of Pretraining</h1><h2 id="The-Concept-of-Pretraining"><a href="#The-Concept-of-Pretraining" class="headerlink" title="The Concept of Pretraining"></a>The Concept of Pretraining</h2>Before, we train our word embeddings first.<br>Then, the vector of each token is fixed through the whole training process.<br><img src="/img/CS224n/截屏2025-10-31 21.33.02.png" alt=""><br>Instead, we can train the whole model parameters.<br>To elaborate, we can train the word embeddings and neural layers jointly.<br>This process is called <strong>Pretraining</strong>.<br><img src="/img/CS224n/截屏2025-10-31 21.36.14.png" alt=""><br>The model can’t be used after pretraining.<br>It’s just a method of initializing the parameters of the model to lay the foundation for the latter training(may be a profile of human language or knowledge of some basic human syntax).<h2 id="The-Pretraining-finetuning-Process"><a href="#The-Pretraining-finetuning-Process" class="headerlink" title="The Pretraining-finetuning Process"></a>The Pretraining-finetuning Process</h2>After the initializing process called pretraining, we can do <strong>Finetuning</strong> to the model, which makes the model be able to handle some task of more specific areas and functions.<br>Starting from the outcome $\hat{\theta}$ of pretraining, we finetune process can go better</li>
</ol>
<ul>
<li>Maybe the finetuning local minima near $\hat{\theta}$ tend to generalize well!</li>
<li>Maybe the gradients of finetuning loss near $\hat{\theta}$ propagate nicely(less explosion or vanishment)!<br><img src="/img/CS224n/截屏2025-10-31 21.46.48.png" alt=""><br>Pretraining costs a lot. But the outcome of the pretraining can be used(finetuned) many times.<br>Finetuning costs far less. It’s common to run finetuning on a single GPU.<h1 id="Pretraining-for-Encoder"><a href="#Pretraining-for-Encoder" class="headerlink" title="Pretraining for Encoder"></a>Pretraining for Encoder</h1>The core method of pretraining encoders is randomly mask some tokens in the source sentence and let the model to predict what masked token is.<h2 id="Bidirectional-Encoder-Representations-from-Transformers-BERT"><a href="#Bidirectional-Encoder-Representations-from-Transformers-BERT" class="headerlink" title="Bidirectional Encoder Representations from Transformers(BERT)"></a>Bidirectional Encoder Representations from Transformers(BERT)</h2>The process is called “Masked LM”.<br>We randomly choose 15% tokens in the source sentence. To the chosen tokens</li>
<li>Replace 80% of them with a special <code>&lt;Mask&gt;</code> token.</li>
<li>Replace 10% of them with another randomly chosen token.</li>
<li>Else 10% we don’t change them as input,but the model still needs to predict them as its output.<br><img src="/img/CS224n/截屏2025-10-31 22.13.06.png" alt=""><br>This teaches the model to make use of context tokens, and find some rules of human language.<h2 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h2>The hidden tokens are randomly chosen for the BERT above.<br>In SpanBERT, we tend to choose adjacent tokens to do some masking.<br>This teaches the model to make use farther and longer contexts, and understand the text structure.<h1 id="Pretraining-for-Encoder-decoder"><a href="#Pretraining-for-Encoder-decoder" class="headerlink" title="Pretraining for Encoder-decoder"></a>Pretraining for Encoder-decoder</h1>The classical process is divide the source sentence into two parts.</li>
<li>Prefix: need to feed into the encoder part with no mask operations.</li>
<li>Target: the task for decoder to predict, and based on the prediction to do back-propagation.<br><img src="/img/CS224n/截屏2025-10-31 23.46.08.png" alt=""><br>Another method used the mask.<br><img src="/img/CS224n/截屏2025-11-01 00.21.04.png" alt=""><br>As the picture shows, we mask some tokens and predict the other tokens.<h1 id="Pretraining-for-Decoder"><a href="#Pretraining-for-Decoder" class="headerlink" title="Pretraining for Decoder"></a>Pretraining for Decoder</h1><em>The Generative Pretrained Transformer(GPT) is a pure Decoder model.</em><h2 id="Target-Output-as-a-Sequence"><a href="#Target-Output-as-a-Sequence" class="headerlink" title="Target Output as a Sequence"></a>Target Output as a Sequence</h2>The precess must be done recurrently.<br><img src="/img/CS224n/截屏2025-11-01 00.48.21.png" alt=""><br>Forward propagation with known tokens <script type="math/tex">w_1,w_2,......,w_t</script>, and generate a hidden layer <script type="math/tex">h_t</script>.<br>Do linear and maybe softmax to <script type="math/tex">h_t</script>, work out the loss and back propagation.<br><img src="/img/CS224n/截屏2025-11-01 00.49.11.png" alt=""><br>Pass the right $w_{t+1}$ to continue training.<h2 id="Other-Downstream-Tasks"><a href="#Other-Downstream-Tasks" class="headerlink" title="Other Downstream Tasks"></a>Other Downstream Tasks</h2>Just recurrently go through all the given tokens, generating a sequence of hidden layers.<br><img src="/img/CS224n/截屏2025-11-01 00.50.39.png" alt=""><br>Use the last hidden layer only.<br>Do linear and softmax to it, and use it to predict some target labels such as the emotion(positive/ negative) of the sentence.<br>The $A$ and $b$ here to do linear are randomly and normally generated.<br><img src="/img/CS224n/截屏2025-11-01 00.53.19.png" alt=""><br>Basing on this, back propagate through the whole network.</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://SCPZ24.github.io">SCPZ24</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/b1811702.html">http://example.com/posts/b1811702.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">SCPZ24-blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/">词嵌入</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a><a class="post-meta__tags" href="/tags/Transformers/">Transformers</a></div><div class="post-share"><div class="social-share" data-image="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/caaad70d.html" title="Embedding"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-13%2019.16.55.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-10-23</div><div class="info-item-2">Embedding</div></div><div class="info-2"><div class="info-item-1">Distributed RepresentationNaturally, we use context to learn about word.So, we can build a dense vector for each word in corpus(word library/vocabulary).By dot product, we can detect the similarity of given two words. Large result stands for large similarity while small one stands for small similarity.Usually, a word vector is 300~500 dimensional. For some modern models, it can be around 1000. Word2VectorWord2Vector is the algorithm to turn words in corpus to vectors.There are two methods to ...</div></div></div></a><a class="pagination-related" href="/posts/26410c69.html" title="预训练后续"><img class="cover" src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-01%2023.18.56.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-11-12</div><div class="info-item-2">预训练后续</div></div><div class="info-2"><div class="info-item-1">Prompting for Pretrained ModelsThis involves no gradient steps!Pretrained models have learned some basic tools and knowledges to handle human language tasks.Take GPT as example. GPT is a decoder that can generated a sequence of tokens based on input tokens(may be human’s questions).When it’s not finetuned for specific tasks, we can still use some prompts(ask it explicitly in the input sentence) to let it handle specific tasks with already learned general skills.The state that the model is tra...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SCPZ24</div><div class="author-info-description">软件工程大二学生，努力成为极客中。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SCPZ24"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的机器学习导论(MIT6.036)和自然语言处理(CS224n)笔记现已上传。欢迎参考交流。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Subword"><span class="toc-number">1.</span> <span class="toc-text">Subword</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Byte-pair-Encode-Algorithm"><span class="toc-number">1.1.</span> <span class="toc-text">Byte-pair Encode Algorithm</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Inspiration-of-Pretraining"><span class="toc-number">2.</span> <span class="toc-text">Inspiration of Pretraining</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Concept-of-Pretraining"><span class="toc-number">2.1.</span> <span class="toc-text">The Concept of Pretraining</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Pretraining-finetuning-Process"><span class="toc-number">2.2.</span> <span class="toc-text">The Pretraining-finetuning Process</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pretraining-for-Encoder"><span class="toc-number">3.</span> <span class="toc-text">Pretraining for Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Bidirectional-Encoder-Representations-from-Transformers-BERT"><span class="toc-number">3.1.</span> <span class="toc-text">Bidirectional Encoder Representations from Transformers(BERT)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SpanBERT"><span class="toc-number">3.2.</span> <span class="toc-text">SpanBERT</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pretraining-for-Encoder-decoder"><span class="toc-number">4.</span> <span class="toc-text">Pretraining for Encoder-decoder</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pretraining-for-Decoder"><span class="toc-number">5.</span> <span class="toc-text">Pretraining for Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Target-Output-as-a-Sequence"><span class="toc-number">5.1.</span> <span class="toc-text">Target Output as a Sequence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Other-Downstream-Tasks"><span class="toc-number">5.2.</span> <span class="toc-text">Other Downstream Tasks</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-12%2011.58.16.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络和树递归神经网络"/></a><div class="content"><a class="title" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络">卷积神经网络和树递归神经网络</a><time datetime="2025-11-12T00:56:00.000Z" title="发表于 2025-11-12 08:56:00">2025-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a7b1f35d.html" title="训练优化"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="训练优化"/></a><div class="content"><a class="title" href="/posts/a7b1f35d.html" title="训练优化">训练优化</a><time datetime="2025-11-07T03:21:00.000Z" title="发表于 2025-11-07 11:21:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8c119005.html" title="HuggingFace入门">HuggingFace入门</a><time datetime="2025-11-02T04:55:00.000Z" title="发表于 2025-11-02 12:55:00">2025-11-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-10-31%2023.46.08.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2026 By SCPZ24</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="/js/tw_cn.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'ams',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.1.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4"></script></div></div></body></html>