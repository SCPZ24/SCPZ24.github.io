<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>训练优化 | SCPZ24-blog</title><meta name="author" content="SCPZ24"><meta name="copyright" content="SCPZ24"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Mixed Precision TrainingTwo Kinds of floatFP16 and FP 32 are all floats. FP16 takes 2 bytes while FP32 takes 4 bytes.Compared to FP16, FP32 has greater range and is preciser. But it takes more memory">
<meta property="og:type" content="article">
<meta property="og:title" content="训练优化">
<meta property="og:url" content="http://example.com/posts/a7b1f35d.html">
<meta property="og:site_name" content="SCPZ24-blog">
<meta property="og:description" content="Mixed Precision TrainingTwo Kinds of floatFP16 and FP 32 are all floats. FP16 takes 2 bytes while FP32 takes 4 bytes.Compared to FP16, FP32 has greater range and is preciser. But it takes more memory">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png">
<meta property="article:published_time" content="2025-11-07T03:21:00.000Z">
<meta property="article:modified_time" content="2025-11-09T14:30:00.000Z">
<meta property="article:author" content="SCPZ24">
<meta property="article:tag" content="LoRA">
<meta property="article:tag" content="多卡训练">
<meta property="article:tag" content="混合精度训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "训练优化",
  "url": "http://example.com/posts/a7b1f35d.html",
  "image": "http://example.com/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png",
  "datePublished": "2025-11-07T03:21:00.000Z",
  "dateModified": "2025-11-09T14:30:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "SCPZ24",
      "url": "https://SCPZ24.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/lpls.png"><link rel="canonical" href="http://example.com/posts/a7b1f35d.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '训练优化',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="bg-animation" id="web_bg" style="background: linear-gradient(20deg, #fffbeb, #ecfdf5);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">SCPZ24-blog</span></a><a class="nav-page-title" href="/"><span class="site-name">训练优化</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">训练优化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon fas fa-history"></i><span class="post-meta-label">更新于</span><time datetime="2025-11-09T14:30:00.000Z" title="更新于 2025-11-09 22:30:00">2025-11-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-CS224n-%E7%AC%94%E8%AE%B0/">自然语言处理(CS224n)笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Mixed-Precision-Training"><a href="#Mixed-Precision-Training" class="headerlink" title="Mixed Precision Training"></a>Mixed Precision Training</h1><h2 id="Two-Kinds-of-float"><a href="#Two-Kinds-of-float" class="headerlink" title="Two Kinds of float"></a>Two Kinds of <code>float</code></h2><p>FP16 and FP 32 are all floats. FP16 takes 2 bytes while FP32 takes 4 bytes.<br>Compared to FP16, FP32 has greater range and is preciser. But it takes more memory and time during computation.<br>It’s common to meet CUDA <code>Out Of Memory</code> exception.<br><img src="/img/CS224n/截屏2025-11-07 11.34.52.png" alt=""><br>But when we are using FP16 throughout the training, some small gradient values will <strong>underflow(become zero)</strong>. We should use both FP16 and FP32.</p>
<h2 id="Mixing-FP32-and-FP16"><a href="#Mixing-FP32-and-FP16" class="headerlink" title="Mixing FP32 and FP16"></a>Mixing FP32 and FP16</h2><p><img src="/img/CS224n/截屏2025-11-07 11.39.52.png" alt=""><br>For origin model parameters, we all use FP32 to store them.<br>When passing to the training process(Forward, Backward and Compute Gradient), we cut them to FP16.<br>We can scale the final loss by a constant factor to further reduce the possibility of underflow.<br>Before doing gradient operations(gradient clipping, updating the original model parameters), we unscale the gradient(divide by scale factor).</p>
<h2 id="Implement-in-PyTorch"><a href="#Implement-in-PyTorch" class="headerlink" title="Implement in PyTorch"></a>Implement in PyTorch</h2><p>We first initialize a <code>GradScaler</code>.<br>We can assign the FWD and Loss Calculation process explicitly<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creates model and optimizer in default precision</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a GradScaler once at the beginning of training.</span></span><br><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Runs the forward pass with autocasting.</span></span><br><span class="line">        <span class="keyword">with</span> autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class="line">        <span class="comment"># Backward passes under autocast are not recommended.</span></span><br><span class="line">        <span class="comment"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.</span></span><br><span class="line">        <span class="comment"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span><br><span class="line">        <span class="comment"># otherwise, optimizer.step() is skipped.</span></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Updates the scale for next iteration.</span></span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure><br>The <code>step</code> function of scaler will automatically <code>unscale</code> the gradient.<br>But if we need to do gradient clipping, we should unscale it explicitly.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Since the gradients of optimizer&#x27;s assigned params are unscaled, clips as usual:</span></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimizer&#x27;s gradients are already unscaled, so scaler.step does not unscale them,</span></span><br><span class="line">        <span class="comment"># although it still skips optimizer.step() if the gradients contain infs or NaNs.</span></span><br><span class="line">        scaler.step(optimizer)</span><br></pre></td></tr></table></figure></p>
<h2 id="BFloat16"><a href="#BFloat16" class="headerlink" title="BFloat16"></a>BFloat16</h2><p><img src="/img/CS224n/截屏2025-11-07 12.01.48.png" alt=""><br>BFloat16 is a special float type.<br>It can express much smaller(closer to 0) or larger values than FP16, for it has 8 Exponents.<br>But it can’t be much precise.<br>Involving BFloat16(replace FP16 with BFloat16), we then need no gradient scaling.</p>
<h1 id="Multi-GPU-Training"><a href="#Multi-GPU-Training" class="headerlink" title="Multi-GPU Training"></a>Multi-GPU Training</h1><h2 id="Distributed-Data-Parallel-DDP"><a href="#Distributed-Data-Parallel-DDP" class="headerlink" title="Distributed Data Parallel(DDP)"></a>Distributed Data Parallel(DDP)</h2><p>When we have a few GPUs, to implement DDP, we store a copy of the whole model parameters in each GPU.<br><img src="/img/CS224n/截屏2025-11-07 21.26.04.png" alt=""><br>When do batched-training, we equally divide the batch into parts.(For example, there are 64 data point in one batch, and we have 8 GPUs, so we assign 8 data point to each GPU)<br>After calculating the loss and the gradient of each GPU’s batches, all GPUs communicate with each other and average each one’s gradient(this process is called “AllReduce”), and do the parameter updating.<br>Then load the next batch and continue training.<br>However, each GPU holds a full copy of model parameters, optimizer state and gradients. It’s a waste of memory.</p>
<h2 id="Fully-Sharded-Data-Parallel-FSDP"><a href="#Fully-Sharded-Data-Parallel-FSDP" class="headerlink" title="Fully Sharded Data Parallel(FSDP)"></a>Fully Sharded Data Parallel(FSDP)</h2><p>FSDP also needs to divide the batch into several parts.<br>GPUs need to communicate with each other, sending values needed by other GPUs.<br><img src="/img/CS224n/截屏2025-11-09 21.17.16.png" alt=""><br>In stage 1(<script type="math/tex">P_{os}</script>) and stage 2(<script type="math/tex">P_{os+g}</script>), we distribute the optimizer states(stage1 and stage 2) and the gradient(stage 2), each GPU only holding a shard of the full datas.<br>These process won’t cause a time cost.<br>To stage 3(<script type="math/tex">P_{os+g+p}</script>), we even distribute the parameters. This will save a lot of memory, but need to cost more time.<br>The full process of FSDP goes as follows.</p>
<ol>
<li>Divide model parameters into FSDP units, and shard each unit across multiple GPUs.<br><img src="/img/CS224n/截屏2025-11-09 21.30.41.png" alt=""></li>
<li>Run forward pass.<ul>
<li>For each layer, perform an all-gather so each GPU gets what it needs.</li>
<li>Run forward pass.</li>
<li>Discard used parts of each GPU.<br><img src="/img/CS224n/截屏2025-11-09 21.33.55.png" alt=""></li>
</ul>
</li>
<li>Run backward pass.<ul>
<li>For each layer, perform an all-gather so each GPU gets what it needs.</li>
<li>Each GPU computes gradient for its data chunk.</li>
<li>Do a reduce-scatter(send full gradient piece to the right GPU).</li>
<li>Each GPU updates its own shard using the full gradient received earlier.<br><img src="/img/CS224n/截屏2025-11-09 21.38.39.png" alt=""><h1 id="Parameter-efficient-Fine-tuning-PEFT"><a href="#Parameter-efficient-Fine-tuning-PEFT" class="headerlink" title="Parameter-efficient Fine-tuning(PEFT)"></a>Parameter-efficient Fine-tuning(PEFT)</h1>To finetune a large model, we need huge memory. So we shall come up with some methods.<br>First we can only choose and finetune only a small part of the model.<br>We do forward propagation with full parameters and only do gradient step to some parameters and fix other parameters.<br><img src="/img/CS224n/截屏2025-11-09 21.39.53.png" alt=""><h2 id="Low-rank-parameterized-Update-LoRA"><a href="#Low-rank-parameterized-Update-LoRA" class="headerlink" title="Low-rank-parameterized Update(LoRA)"></a>Low-rank-parameterized Update(LoRA)</h2>When doing weight update, the updating matrix usually is a low rank matrix.<br>For a weight matrix $W_0$, we do<script type="math/tex; mode=display">
W'=W_0+\Delta W</script>in which, Three matrices are $d\times k$.<br>Notice that $W_0$ is weight matrix after pretraining. $\Delta W$ is the overall difference between the pretrained weight and the finetuned weight.<br>And here, $\Delta W$ is usually a low-rank matrix.<br>So, we can view it as<script type="math/tex; mode=display">
\Delta W=BA</script>in which, $B\in R^{d\times r}$ and $A\in R^{r\times k}$.<br>$r$ is far small than $\min(d,k)$, causing the number of parameters to be update greatly reduced.<br>While finetuning, we fix $W_0$ and only modify $B$ and $A$.<br>After the training process, we do update with hyperparameter $\alpha$<script type="math/tex; mode=display">
W_+=W_0+\alpha BA</script>in which, $\alpha$ is a scalar that do a trade-off between original knowledge learned from pretraining and new knowledge learned from finetuning.<h1 id="Strategy-Selection-in-Efficient-Training"><a href="#Strategy-Selection-in-Efficient-Training" class="headerlink" title="Strategy Selection in Efficient Training"></a>Strategy Selection in Efficient Training</h1>We shall use different efficient training methods, for our GPU power and memory are always finite.<br><img src="/img/CS224n/截屏2025-11-09 22.30.24.png" alt=""></li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://SCPZ24.github.io">SCPZ24</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/a7b1f35d.html">http://example.com/posts/a7b1f35d.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">SCPZ24-blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/">混合精度训练</a><a class="post-meta__tags" href="/tags/%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/">多卡训练</a><a class="post-meta__tags" href="/tags/LoRA/">LoRA</a></div><div class="post-share"><div class="social-share" data-image="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/OldWu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SCPZ24</div><div class="author-info-description">软件工程大二学生，努力成为极客中。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SCPZ24"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的机器学习导论(MIT6.036)和自然语言处理(CS224n)笔记现已上传。欢迎参考交流。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Mixed-Precision-Training"><span class="toc-number">1.</span> <span class="toc-text">Mixed Precision Training</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Two-Kinds-of-float"><span class="toc-number">1.1.</span> <span class="toc-text">Two Kinds of float</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mixing-FP32-and-FP16"><span class="toc-number">1.2.</span> <span class="toc-text">Mixing FP32 and FP16</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implement-in-PyTorch"><span class="toc-number">1.3.</span> <span class="toc-text">Implement in PyTorch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BFloat16"><span class="toc-number">1.4.</span> <span class="toc-text">BFloat16</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Multi-GPU-Training"><span class="toc-number">2.</span> <span class="toc-text">Multi-GPU Training</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Distributed-Data-Parallel-DDP"><span class="toc-number">2.1.</span> <span class="toc-text">Distributed Data Parallel(DDP)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fully-Sharded-Data-Parallel-FSDP"><span class="toc-number">2.2.</span> <span class="toc-text">Fully Sharded Data Parallel(FSDP)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Parameter-efficient-Fine-tuning-PEFT"><span class="toc-number">3.</span> <span class="toc-text">Parameter-efficient Fine-tuning(PEFT)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Low-rank-parameterized-Update-LoRA"><span class="toc-number">3.1.</span> <span class="toc-text">Low-rank-parameterized Update(LoRA)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Strategy-Selection-in-Efficient-Training"><span class="toc-number">4.</span> <span class="toc-text">Strategy Selection in Efficient Training</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-12%2011.58.16.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络和树递归神经网络"/></a><div class="content"><a class="title" href="/posts/4a42f569.html" title="卷积神经网络和树递归神经网络">卷积神经网络和树递归神经网络</a><time datetime="2025-11-12T00:56:00.000Z" title="发表于 2025-11-12 08:56:00">2025-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a7b1f35d.html" title="训练优化"><img src="/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="训练优化"/></a><div class="content"><a class="title" href="/posts/a7b1f35d.html" title="训练优化">训练优化</a><time datetime="2025-11-07T03:21:00.000Z" title="发表于 2025-11-07 11:21:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8c119005.html" title="HuggingFace入门">HuggingFace入门</a><time datetime="2025-11-02T04:55:00.000Z" title="发表于 2025-11-02 12:55:00">2025-11-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/CS224n/%E6%88%AA%E5%B1%8F2025-11-07%2021.26.04.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2026 By SCPZ24</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="/js/tw_cn.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'ams',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.1.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4"></script></div></div></body></html>